{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/intro.004.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/intro.005.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/intro.006.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/intro.007.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/intro.009.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/intro.010.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resilient Distributed Datasets (RDDs)\n",
    "* Basic abstraction in Spark. Fault-tolerant collection of elements that can be operated on in parallel\n",
    "\n",
    "* RDDs can be created from local file system, HDFS, Cassandra, HBase, Amazon S3, SequenceFiles, and any other Hadoop InputFormat.\n",
    "\n",
    "* Different levels of caching: MEMORY_ONLY, MEMORY_AND_DISK, DISK_ONLY, OFF_HEAP, etc\n",
    "\n",
    "* Rich APIs for Transformations and Actions\n",
    "\n",
    "* Data Locality: PROCESS_LOCAL -> NODE_LOCAL -> RACK_LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x10ed7df90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, HiveContext\n",
    "\n",
    "import py4j\n",
    "\n",
    "conf = SparkConf().setAppName(\"IntroSparkJupyter\") \\\n",
    "         .setMaster(\"local[2]\") \n",
    "# conf = SparkConf().setAppName(\"IntroSparkJupyter\") \\\n",
    "#         .setMaster(\"yarn-client\") \\\n",
    "#         .set(\"spark.executor.memory\", \"512m\") \\\n",
    "#         .set(\"spark.executor.cores\", 1) \\\n",
    "#         .set(\"spark.executor.instances\", 2)\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "try:\n",
    "    # Try to access HiveConf, it will raise exception if Hive is not added\n",
    "    sc._jvm.org.apache.hadoop.hive.conf.HiveConf()\n",
    "    sqlContext = HiveContext(sc)\n",
    "except py4j.protocol.Py4JError:\n",
    "    sqlContext = SQLContext(sc)\n",
    "except TypeError:\n",
    "    sqlContext = SQLContext(sc)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(xrange(10, 0, -1)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sample(False, 0.2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortBy(lambda x: x, ascending=True).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 9, 8, 7, 6]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda x: x>5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 18, 16, 14, 12, 10, 8, 6, 4, 2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x*2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd.reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate value for pi:  3.14458\n",
      "Difference to exact value of pi:  0.00298734641021\n",
      "Error: (approx-exact)/exact= 0.0950901895824 %\n"
     ]
    }
   ],
   "source": [
    "# Calculating Pi with the Monte Carlo method\n",
    "# https://learntofish.wordpress.com/2010/10/13/calculating-pi-with-the-monte-carlo-method/\n",
    "\n",
    "import random\n",
    "import math\n",
    " \n",
    "def withinCircle(x,y):\n",
    "    if(x**2+y**2<1):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    " \n",
    "def main():\n",
    "    circleArea = 0\n",
    "    squareArea = 0\n",
    "    pi = 0\n",
    "    for i in range(0,1000000):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if(withinCircle(x,y)==1):\n",
    "                   circleArea=circleArea+1\n",
    "        squareArea=squareArea+1\n",
    "    pi = 4.0*circleArea/squareArea\n",
    "    print \"Approximate value for pi: \", pi\n",
    "    print \"Difference to exact value of pi: \", pi-math.pi\n",
    "    print \"Error: (approx-exact)/exact=\", (pi-math.pi)/math.pi*100, \"%\"\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/circle_and_square.png)\n",
    "![](images/Screen Shot 2016-03-07 at 9.40.45 pm.png)\n",
    "![](images/pythagoras-theorem.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.141916\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "import math\n",
    "\n",
    "def f(_):\n",
    "    x = random() \n",
    "    y = random() \n",
    "    if x ** 2 + y ** 2 < 1: \n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "n = 1000000\n",
    "count = sc.parallelize(xrange(1, n)).map(f).reduce(add)\n",
    "print(\"Pi is roughly {}\".format( 4.0 * count / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Spark 1.6.0 (Python 2.7.11)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
